{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653ea5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Claire Wagner\n",
    "# Date: 15 June 2022\n",
    "# Purpose: To generate data about city-owned PINs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d447d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import datetime\n",
    "import json\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad284ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_token = \"yo0POz8pPZyyDO9jOvtesb42J\" # CW - HEI Leaflet Map App Token\n",
    "limit = 3000000\n",
    "city_owned_api_info = { \"domain\" : \"https://data.cityofchicago.org\", \"dataset\" : \"aksk-kvfp\" }\n",
    "properties_api_info = { \"domain\" : \"https://datacatalog.cookcountyil.gov\", \"dataset\" : \"c49d-89sn\" }\n",
    "business_licenses_api_info = { \"domain\" : \"https://data.cityofchicago.org\", \"dataset\" : \"uupf-x98q\" }\n",
    "ward_boundaries_2015_to_2023_api_info = { \"domain\" : \"https://data.cityofchicago.org\", \"dataset\" : \"k9yb-bpqx\" }\n",
    "ward_boundaries_2023_api_info = { \"domain\" : \"https://data.cityofchicago.org\", \"dataset\" : \"p293-wvbd\" }\n",
    "neighborhood_boundaries_api_info = { \"domain\" : \"https://data.cityofchicago.org\", \"dataset\" : \"y6yq-dbs2\" }\n",
    "zoning_districts_api_info = { \"domain\" : \"https://data.cityofchicago.org\", \"dataset\" : \"dj47-wfun\" }\n",
    "ward_offices_api_info = { \"domain\" : \"https://data.cityofchicago.org\", \"dataset\" : \"htai-wnw4\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b23473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate attribution string that gives the URLs and the access date and time for the data sources.\n",
    "fetchtime = datetime.datetime.now(datetime.timezone.utc).strftime(\"%d %b %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a74478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeAPIRequest(api_endpoint, params, limit, read_function):\n",
    "    \"\"\"Helper function to make Socrata API request.\"\"\"\n",
    "    query = \"?\"\n",
    "    if len(params) > 0:\n",
    "        query += \"&\".join(params) + \"&\"\n",
    "    query += \"$limit=\" + str(limit) + \"&$$app_token=\" + app_token\n",
    "    return read_function(api_endpoint + urllib.parse.quote(query, safe=\"&?$=,!()\"))\n",
    "\n",
    "def get_url_response(url):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        if response.status == 200:\n",
    "            return response.read()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def get_metadata(api_info):\n",
    "    def get_original_metadata(domain, dataset, actual_datasource_uri = None):\n",
    "        original_metadata = json.loads(makeAPIRequest(\n",
    "            api_endpoint = domain + \"/api/views/metadata/v1/\" + dataset + \".json\",\n",
    "            params = [],\n",
    "            limit = limit,\n",
    "            read_function = get_url_response\n",
    "        ))\n",
    "        # if this isn't the full metadata for this dataset, use the provided \"reviewableUid\" as the metadata\n",
    "        # while providing information about the actual original datasource\n",
    "        if \"approvals\" in original_metadata and \"reviewableUid\" in original_metadata[\"approvals\"][-1]:\n",
    "            print(f\"reviewableUid found for {original_metadata['name']}\")\n",
    "            return get_original_metadata(\n",
    "                domain,\n",
    "                original_metadata[\"approvals\"][-1][\"reviewableUid\"],\n",
    "                original_metadata[\"actualDatasourceUri\"] if \"actualDatasourceUri\" in original_metadata else original_metadata[\"dataUri\"],\n",
    "            )\n",
    "        else:\n",
    "            # we've found the correct metadata to use, so include actual_datasource_uri in the metadata\n",
    "            if actual_datasource_uri != None:\n",
    "                original_metadata[\"actualDatasourceUri\"] = actual_datasource_uri\n",
    "            return original_metadata\n",
    "    # get metadata\n",
    "    original_metadata = get_original_metadata(api_info[\"domain\"], api_info[\"dataset\"])\n",
    "    # edit metadata for return\n",
    "    edited_metadata = { key: original_metadata[key] if key in original_metadata else None for key in [\n",
    "        \"id\", \"name\", \"description\", \"dataUri\", \"attribution\", \"attributionLink\", \"actualDatasourceUri\",\n",
    "    ] }\n",
    "    for key in [\"createdAt\", \"dataUpdatedAt\", \"metadataUpdatedAt\"]:\n",
    "        if (key not in original_metadata or original_metadata[key] == None):\n",
    "            edited_metadata[key] = None\n",
    "        else:\n",
    "            edited_metadata[key] = datetime.datetime.fromisoformat(original_metadata[key].split('T')[0]).strftime(\"%d %b %Y\")\n",
    "    edited_metadata[\"accessedOn\"] = fetchtime\n",
    "    return edited_metadata\n",
    "\n",
    "def standardize_columns(df):\n",
    "    \"\"\"standardize the names of all columns in df by making them lowercase and snake_case\"\"\"\n",
    "    df.columns = df.columns.str.lower().str.split().str.join('_')\n",
    "    \n",
    "def get_dataset_url(api_info, suffix):\n",
    "    return api_info[\"domain\"] + \"/resource/\" + api_info[\"dataset\"] + \".\" + suffix\n",
    "\n",
    "def get_geojson_data(api_info):\n",
    "    output = {}\n",
    "    output[\"data\"] = json.loads(makeAPIRequest(\n",
    "        api_endpoint = get_dataset_url(api_info, \"geojson\"),\n",
    "        params = [],\n",
    "        limit = limit,\n",
    "        read_function = get_url_response\n",
    "    ))\n",
    "    output[\"metadata\"] = get_metadata(api_info)\n",
    "    return output\n",
    "\n",
    "def set_aggregation_function(x):\n",
    "    \"\"\"Aggregate x by returning a list of all unique values in x in the order they were encountered\n",
    "    (or, if there is only one unique value in x, returning that value)\"\"\"\n",
    "    xSet = list(OrderedDict.fromkeys(x).keys())\n",
    "    return xSet[0] if len(xSet) == 1 else xSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "849187ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data from the Property Locations dataset about all properties in Wards 1-50\n",
    "properties = makeAPIRequest(\n",
    "    api_endpoint = get_dataset_url(properties_api_info, \"json\"),\n",
    "    params = [\n",
    "        \"$select=pin, property_address, property_zip, ward, longitude, latitude, tract_geoid\",\n",
    "        \"$where=(latitude IS NOT NULL) AND (longitude IS NOT NULL)\",\n",
    "    ],\n",
    "    limit = limit,\n",
    "    read_function = pd.read_json,\n",
    ")\n",
    "properties_metadata = get_metadata(properties_api_info)\n",
    "properties_metadata[\"dataUseNotes\"] = \"Used to obtain location information for PINs in Cook County.\"\n",
    "\n",
    "properties[\"pin\"] = properties[\"pin\"].apply(str).str.rjust(14, '0')\n",
    "\n",
    "with open(\"../data/misc/misc.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"const property_locations_metadata = \")\n",
    "    f.write(json.dumps(properties_metadata) + \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee6c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ward_boundaries():\n",
    "    print(\"Getting data for ward boundaries\")\n",
    "    with open(\"../data/geojson/ward_boundaries.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"const ward_boundaries_2015_to_2023 = \")\n",
    "        f.write(json.dumps(get_geojson_data(ward_boundaries_2015_to_2023_api_info)) + \";\")\n",
    "        f.write(\"\\n\\nconst ward_boundaries_2023 = \")\n",
    "        f.write(json.dumps(get_geojson_data(ward_boundaries_2023_api_info)) + \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f35ac128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighborhood_boundaries():\n",
    "    print(\"Getting data for neighborhood boundaries\")\n",
    "    with open(\"../data/geojson/neighborhood_boundaries.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"const neighborhood_boundaries = \")\n",
    "        f.write(json.dumps(get_geojson_data(neighborhood_boundaries_api_info)) + \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c634897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zoning_districts():\n",
    "    print(\"Getting data for zoning districts\")\n",
    "    with open(\"../data/geojson/zoning_districts.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"const zoning_districts = \")\n",
    "        f.write(json.dumps(get_geojson_data(zoning_districts_api_info)) + \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2331246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_properties():\n",
    "    print(\"Getting data for individual properties\")\n",
    "    # fetch location info for Sunshine Gospel Ministries address (source: https://www.sunshinegospel.org/)\n",
    "    sgmAddress = \"500 E 61st St\".lower()\n",
    "    # get location data for Sunshine Gospel Ministries\n",
    "    sgm = makeAPIRequest(\n",
    "        api_endpoint = get_dataset_url(properties_api_info, \"json\"),\n",
    "        params = [\n",
    "            \"$select=pin, longitude, latitude\",\n",
    "            f\"$where=lower(property_address)='{sgmAddress}'\",\n",
    "        ],\n",
    "        limit = 1,\n",
    "        read_function = pd.read_json,\n",
    "    ).loc[0]\n",
    "    \n",
    "    ward_20_office = makeAPIRequest(\n",
    "        api_endpoint = get_dataset_url(ward_offices_api_info, \"json\"),\n",
    "        params = [\n",
    "            \"$select=ward, alderman, address, location\",\n",
    "            \"$where=ward=20\",\n",
    "        ],\n",
    "        limit = 1,\n",
    "        read_function = pd.read_json,\n",
    "    ).loc[0]\n",
    "    \n",
    "    ward_offices_metadata = get_metadata(ward_offices_api_info)\n",
    "    ward_offices_metadata[\"dataUseNotes\"] = \"Used to obtain location information for the Ward 20 office.\"\n",
    "        \n",
    "    with open(\"../data/misc/individual_properties.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"const sunshine_gospel = \")\n",
    "        f.write(sgm.to_json(orient=\"index\") + \";\")\n",
    "        f.write(\"\\n\\nconst ward_20_office = \")\n",
    "        f.write(ward_20_office.to_json(orient=\"index\") + \";\")\n",
    "        f.write(\"\\n\\nconst ward_offices_metadata = \")\n",
    "        f.write(json.dumps(ward_offices_metadata) + \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae2fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_owned():\n",
    "    print(\"Getting data for city-owned properties\")\n",
    "    # fetch data from the City-Owned Land Inventory dataset about all properties currently owned by the City of Chicago that might be up for sale (see http://dev.cityofchicago.org/open%20data/data%20portal/2020/08/11/city-owned-property.html)\n",
    "    city_owned = makeAPIRequest(\n",
    "        api_endpoint = get_dataset_url(city_owned_api_info, \"json\"),\n",
    "        params = [\n",
    "            \"$select=pin, managing_organization, lower(property_status) AS property_status, date_of_acquisition, date_of_disposition, sq_ft, last_update\",\n",
    "            \"$where=(lower(property_status)='owned by city') AND (lower(managing_organization)='none' OR managing_organization IS NULL)\",\n",
    "        ],\n",
    "        limit = limit,\n",
    "        read_function = pd.read_json,\n",
    "    )\n",
    "    city_owned_metadata = get_metadata(city_owned_api_info)\n",
    "    \n",
    "    city_owned[\"pin\"] = city_owned[\"pin\"].str.replace(\"-\",\"\")\n",
    "    city_owned_join = pd.merge(city_owned, properties, how=\"inner\", on=\"pin\", suffixes = [\"_aksk-kvfp\", \"_c49d-89sn\"])\n",
    "    count_of_no_updated_location_info = city_owned.shape[0] - city_owned_join.shape[0]\n",
    "    print(\"number of city-owned properties for which no location data from properties could be found:\", count_of_no_updated_location_info)\n",
    "\n",
    "    city_owned_metadata[\"dataUseNotes\"] = 'Only PINs where \"property_status\" is \"owned by city\" and' \\\n",
    "    + ' \"managing_organization\" is \"none\" or blank (using case-insensitive matching) have been included on the map' \\\n",
    "    + ' (based on the Open Data Portal Team\\'s' \\\n",
    "    + ' <a href=\"http://dev.cityofchicago.org/open%20data/data%20portal/2020/08/11/city-owned-property.html\">notes</a>' \\\n",
    "    + ' about the dataset). Up-to-date location information was obtained using the' \\\n",
    "    + f' <a href={properties_metadata[\"dataUri\"]}>\"{properties_metadata[\"name\"]}\"</a> dataset' \\\n",
    "    + f' ({count_of_no_updated_location_info} PINs for which no up-to-date location information could be found have been excluded).'\n",
    "    \n",
    "    city_owned_join.filter(items=[\n",
    "        \"pin\",\n",
    "        \"managing_organization\",\n",
    "        \"property_status\",\n",
    "        \"last_update\",\n",
    "        \"date_of_acquisition\",\n",
    "        \"date_of_disposition\",\n",
    "        \"property_address\",\n",
    "        \"property_zip\",\n",
    "        \"ward\",\n",
    "        \"tract_geoid\",\n",
    "    ]).to_excel(\"city_owned_pins_possibly_for_sale.xlsx\", index=False)\n",
    "\n",
    "    city_owned_join = city_owned_join.set_index(\"pin\", drop=False)\n",
    "    \n",
    "    with open(\"../data/city-owned/city_owned_data.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"const city_owned_data = \")\n",
    "        f.write(city_owned_join.to_json(orient=\"index\") + \";\")\n",
    "        f.write(\"\\n\\nconst city_owned_metadata = \")\n",
    "        f.write(json.dumps(city_owned_metadata) + \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b2885cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_licenses():\n",
    "    print(\"Getting data for business licenses\")\n",
    "    business_licenses = makeAPIRequest(\n",
    "        api_endpoint = get_dataset_url(business_licenses_api_info, \"json\"),\n",
    "        params = [\n",
    "            \"$select=license_number, license_id AS license_record_id, doing_business_as_name, license_description, business_activity, address, ward, zip_code, longitude, latitude, license_start_date\",\n",
    "            \"$where=city='CHICAGO' AND (latitude IS NOT NULL) AND (longitude IS NOT NULL)\",\n",
    "        ],\n",
    "        limit = limit,\n",
    "        read_function = pd.read_json,\n",
    "    )\n",
    "    business_licenses_metadata = get_metadata(business_licenses_api_info)\n",
    "    business_licenses_metadata[\"dataUseNotes\"] = 'Only PINs with valid coordinates where \"city\" is \"CHICAGO\" have been included on the map.'\n",
    "    \n",
    "    # filter out duplicate entries for the same license number, keeping only the entry with the most recent license start date\n",
    "    business_licenses_filtered = business_licenses.sort_values(by=['license_number', 'license_start_date']).drop_duplicates(subset=['license_number'], keep='last').drop(columns=['license_start_date'])\n",
    "    # check that each license number is unique\n",
    "    assert business_licenses_filtered.shape[0] == business_licenses_filtered['license_number'].unique().shape[0]\n",
    "    \n",
    "    # aggregate licenses into a single entry for each address of each business\n",
    "    business_licenses_final = business_licenses_filtered.groupby(\"doing_business_as_name\", \"address\").agg(set_aggregation_function).reset_index()\n",
    "    \n",
    "    with open(\"../data/business-licenses/business_license_data.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"const business_license_data = \")\n",
    "        f.write(business_licenses_final.to_json(orient=\"records\") + \";\")\n",
    "        f.write(\"\\n\\nconst business_licenses_metadata = \")\n",
    "        f.write(json.dumps(business_licenses_metadata) + \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3d4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scavenger_sale():\n",
    "    print(\"Getting data for scavenger sale\")\n",
    "    # scavenger sale data from 28 January 2022 (source: https://web.archive.org/web/20220128184252/https://www.cookcountytreasurer.com/pdfs/scavsale/2022cookcountyscavengertaxsalelist.xlsx, accessed 3 June 2022)\n",
    "    scavenger = pd.read_excel(\"../data/scavenger-sale/2022cookcountyscavengertaxsalelist_1-28-22.xlsx\")\n",
    "    standardize_columns(scavenger)\n",
    "    scavenger[\"pin\"] = scavenger[\"pin\"].str.replace(\"-\", \"\")\n",
    "    scavenger[\"classification\"] = scavenger[\"classification\"].str.strip()\n",
    "    \n",
    "    # property class data (source: https://www.cookcountyassessor.com/form-document/codes-classification-property, accessed 22 June 2022)\n",
    "    property_classes = pd.read_excel(\"../data/scavenger-sale/property_classes.xlsx\")\n",
    "    # join scavenger sale data with property class data\n",
    "    scavenger_with_property_classes = pd.merge(scavenger, property_classes, how=\"left\", left_on = \"classification\", right_on=\"property_code\").drop(columns=['classification'])\n",
    "    # make sure all scavenger sale entries have associated property class data\n",
    "    assert scavenger_with_property_classes[pd.isna(scavenger_with_property_classes['property_code'])].shape[0] == 0\n",
    "    \n",
    "    # get location data for pins on scavenger sale list\n",
    "    scavenger_final_join = pd.merge(scavenger_with_property_classes, properties, how=\"inner\", on=\"pin\", suffixes=[\"_scavenger_sale\", \"_c49d-89sn\"])\n",
    "    # make sure all pins on scavenger sale list have associated location data\n",
    "    assert scavenger_final_join.shape[0] == scavenger_with_property_classes.shape[0]\n",
    "    scavenger_final_join = scavenger_final_join[scavenger_final_join['property_city'] == 'CHICAGO'].filter(items = [\n",
    "        'pin',\n",
    "        'property_address_scavenger_sale',\n",
    "        'ward',\n",
    "        'delinquent_tax_year_range',\n",
    "        'delinquent_tax',\n",
    "        'delinquent_interest',\n",
    "        'total_delinquency',\n",
    "        '2020_taxes_billed',\n",
    "        'property_code',\n",
    "        'property_code_meaning',\n",
    "        'property_code_class',\n",
    "        'property_zip',\n",
    "        'longitude',\n",
    "        'latitude',\n",
    "        'tract_geoid',\n",
    "    ]).rename(columns = {\n",
    "        'property_address_scavenger_sale' : 'property_address',\n",
    "    }).set_index('pin', drop=False)\n",
    "    # make sure there are no duplicated entries for pins\n",
    "    assert scavenger_final_join.index.unique().shape[0] == scavenger_final_join.index.shape[0]\n",
    "    \n",
    "    scavenger_sale_metadata = {\n",
    "        'id': None,\n",
    "        'name': '2022 Cook County Scavenger Tax Sale List',\n",
    "        'description': 'Properties scheduled to be offered at the 2022 Scavenger Sale.',\n",
    "        'dataUri': 'https://web.archive.org/web/20220128181637/https://www.cookcountytreasurer.com/scavengersalemap.aspx',\n",
    "        'attribution': 'Cook County Treasurer',\n",
    "        'attributionLink': 'https://www.cookcountytreasurer.com/',\n",
    "        'actualDatasourceUri': 'https://web.archive.org/web/20220128184252/https://www.cookcountytreasurer.com/pdfs/scavsale/2022cookcountyscavengertaxsalelist.xlsx',\n",
    "        'createdAt': None,\n",
    "        'dataUpdatedAt': '28 January 2022',\n",
    "        'metadataUpdatedAt': None,\n",
    "        'accessedOn': '3 Jun 2022',\n",
    "        'dataUseNotes': 'Only PINs where \"property_city\" is \"CHICAGO\" have been included on the map.' \\\n",
    "        + ' Location information was obtained using the' \\\n",
    "        + f' <a href={properties_metadata[\"dataUri\"]}>\"{properties_metadata[\"name\"]}\"</a> dataset.'\n",
    "    }\n",
    "    \n",
    "    property_classification_codes_metadata = {\n",
    "        'id': None,\n",
    "        'name': 'Definitions for the Codes for Classification of Real Property',\n",
    "        'description': 'Definitions for the codes for classification of real property.',\n",
    "        'dataUri': 'https://prodassets.cookcountyassessor.com/s3fs-public/form_documents/classcode.pdf?VersionId=12JVr.oX..WD4hfgjLCp5AIVfar71ndn',\n",
    "        'attribution': 'Cook County Assessor',\n",
    "        'attributionLink': 'https://www.cookcountyassessor.com/',\n",
    "        'actualDatasourceUri': None,\n",
    "        'createdAt': '03 April 2018',\n",
    "        'dataUpdatedAt': None,\n",
    "        'metadataUpdatedAt': None,\n",
    "        'accessedOn': '22 Jun 2022',\n",
    "    }\n",
    "    \n",
    "    with open(\"../data/scavenger-sale/scavenger_data.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"const scavenger_sale_data = \") # assign join JSON data to variable for easier access by JavaScript scripts in browser\n",
    "        f.write(scavenger_final_join.to_json(orient=\"index\") + \";\") # output as JSON\n",
    "        f.write(\"\\n\\nconst scavenger_sale_metadata = \")\n",
    "        f.write(json.dumps(scavenger_sale_metadata) + \";\")\n",
    "        f.write(\"\\n\\nconst property_classification_codes_metadata = \")\n",
    "        f.write(json.dumps(property_classification_codes_metadata) + \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "954745a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tax_sale_ward_20():\n",
    "    print(\"Getting data for Ward 20 tax sale\")\n",
    "    # Ward 20 Properties on Tax Sale list for Tax Year 2019, accessed on 17 May 2022 around 11 am ET\n",
    "    taxsale = pd.read_excel(\"../data/tax-sale/tax_sale_tax_year_2019_ward20_accessed_17_May_2022.xlsx\")\n",
    "    standardize_columns(taxsale)\n",
    "    taxsale['pin'] = taxsale['pin'].str.replace('-', '')\n",
    "    taxsale = taxsale.filter(items=[\n",
    "        'pin',\n",
    "        'property_address',\n",
    "        'current_mailing_address',\n",
    "        'taxpayer_name',\n",
    "        'tax_type',\n",
    "        'tax_year',\n",
    "        'total_tax_due',\n",
    "        'total_due_(including_interest)',\n",
    "        'classification',\n",
    "        'prior_tax_years_may_also_be_unpaid',\n",
    "    ])\n",
    "    taxsale = taxsale[taxsale['tax_year'] == 2019]\n",
    "    # make sure no entries have duplicate pins\n",
    "    assert taxsale['pin'].shape[0] == taxsale['pin'].unique().shape[0]\n",
    "    \n",
    "    taxsale_join = pd.merge(taxsale, properties, how=\"inner\", on=\"pin\", suffixes=[\"_tax_sale\", \"_c49d-89sn\"])\n",
    "    # make sure all pins have associated location data and are in Ward 20\n",
    "    assert taxsale_join.shape[0] == taxsale.shape[0]\n",
    "    assert taxsale_join.shape[0] == taxsale_join[taxsale_join['ward'] == 20].shape[0]\n",
    "    taxsale_join = taxsale_join.drop(columns=[\n",
    "        'property_address_c49d-89sn',\n",
    "    ]).rename(columns={\n",
    "        'property_address_tax_sale' : 'property_address',\n",
    "        'total_due_(including_interest)' : 'total_due_including_interest',\n",
    "    }).set_index('pin', drop=False)\n",
    "    \n",
    "    taxsale_ward20_metadata = {\n",
    "        'id': None,\n",
    "        'name': 'Ward 20 Delinquent Tax Report',\n",
    "        'description': 'Properties in Ward 20 with delinquent taxes for Tax Year 2019 (payable in 2020) that were eligible for the Annual Tax Sale that began May 12, 2022.',\n",
    "        'dataUri': 'https://www.cookcountytreasurer.com/delinquenttaxes.aspx',\n",
    "        'attribution': 'Cook County Treasurer',\n",
    "        'attributionLink': 'https://www.cookcountytreasurer.com/',\n",
    "        'actualDatasourceUri': None,\n",
    "        'createdAt': None,\n",
    "        'dataUpdatedAt': None,\n",
    "        'metadataUpdatedAt': None,\n",
    "        'accessedOn': '17 Jun 2022',\n",
    "        'dataUseNotes': 'Only PINs where \"tax_year\" is 2019 have been included on the map.' \\\n",
    "        + ' Location information was obtained using the' \\\n",
    "        + f' <a href={properties_metadata[\"dataUri\"]}>\"{properties_metadata[\"name\"]}\"</a> dataset.'\n",
    "    }\n",
    "    \n",
    "    with open(\"../data/tax-sale/tax_sale_ward20_tax_year_2019.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"const taxsale_ward20_data = \") # assign JSON data to variable for easier access by JavaScript scripts in browser\n",
    "        f.write(taxsale_join.to_json(orient=\"index\") + \";\") # output as JSON\n",
    "        f.write(\"\\n\\nconst taxsale_ward20_metadata = \")\n",
    "        f.write(json.dumps(taxsale_ward20_metadata) + \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75dd9f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for business licenses\n"
     ]
    }
   ],
   "source": [
    "    print(\"Getting data for business licenses\")\n",
    "    business_licenses = makeAPIRequest(\n",
    "        api_endpoint = get_dataset_url(business_licenses_api_info, \"json\"),\n",
    "        params = [\n",
    "            \"$select=license_number, license_id AS license_record_id, doing_business_as_name, license_description, business_activity, address, ward, zip_code, longitude, latitude, license_start_date\",\n",
    "            \"$where=city='CHICAGO' AND (latitude IS NOT NULL) AND (longitude IS NOT NULL)\",\n",
    "        ],\n",
    "        limit = limit,\n",
    "        read_function = pd.read_json,\n",
    "    )\n",
    "    business_licenses_metadata = get_metadata(business_licenses_api_info)\n",
    "    business_licenses_metadata[\"dataUseNotes\"] = 'Only PINs with valid coordinates where \"city\" is \"CHICAGO\" have been included on the map.'\n",
    "    \n",
    "    # filter out duplicate entries for the same license number, keeping only the entry with the most recent license start date\n",
    "    business_licenses_filtered = business_licenses.sort_values(by=['license_number', 'license_start_date']).drop_duplicates(subset=['license_number'], keep='last').drop(columns=['license_start_date'])\n",
    "    # check that each license number is unique\n",
    "    assert business_licenses_filtered.shape[0] == business_licenses_filtered['license_number'].unique().shape[0]\n",
    "    \n",
    "    # aggregate licenses into a single entry for each address of each business\n",
    "    #business_licenses_final = business_licenses_filtered.groupby(\"doing_business_as_name\", \"address\").agg(set_aggregation_function).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0af8e0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51482, 11)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_licenses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98a90ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50136, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_licenses_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10c654e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for ward boundaries\n",
      "reviewableUid found for WARDS_2015\n",
      "Getting data for neighborhood boundaries\n",
      "reviewableUid found for Neighborhoods_2012b\n",
      "Getting data for zoning districts\n",
      "reviewableUid found for zoning_2016_01\n",
      "Getting data for individual properties\n",
      "Getting data for city-owned properties\n",
      "number of city-owned properties for which no location data from properties could be found: 6\n",
      "Getting data for business licenses\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No axis named address for object type DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py:550\u001b[0m, in \u001b[0;36mNDFrame._get_axis_number\u001b[0;34m(cls, axis)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_AXIS_TO_AXIS_NUMBER\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'address'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m get_individual_properties()\n\u001b[1;32m      6\u001b[0m get_city_owned()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mget_business_licenses\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m get_scavenger_sale()\n\u001b[1;32m      9\u001b[0m get_tax_sale_ward_20()\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mget_business_licenses\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m business_licenses_filtered\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m business_licenses_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlicense_number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# aggregate licenses into a single entry for each address of each business\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m business_licenses_final \u001b[38;5;241m=\u001b[39m \u001b[43mbusiness_licenses_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdoing_business_as_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maddress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39magg(set_aggregation_function)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/business-licenses/business_license_data.js\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     24\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconst business_license_data = \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py:7707\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   7705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   7706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7707\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_axis_number\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7709\u001b[0m \u001b[38;5;66;03m# https://github.com/python/mypy/issues/7642\u001b[39;00m\n\u001b[1;32m   7710\u001b[0m \u001b[38;5;66;03m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[39;00m\n\u001b[1;32m   7711\u001b[0m \u001b[38;5;66;03m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[39;00m\n\u001b[1;32m   7712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[1;32m   7713\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   7714\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   7722\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[1;32m   7723\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py:552\u001b[0m, in \u001b[0;36mNDFrame._get_axis_number\u001b[0;34m(cls, axis)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_AXIS_TO_AXIS_NUMBER[axis]\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 552\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo axis named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for object type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No axis named address for object type DataFrame"
     ]
    }
   ],
   "source": [
    "# comment out the functions that should not be called\n",
    "get_ward_boundaries()\n",
    "get_neighborhood_boundaries()\n",
    "get_zoning_districts()\n",
    "get_individual_properties()\n",
    "get_city_owned()\n",
    "get_business_licenses()\n",
    "get_scavenger_sale()\n",
    "get_tax_sale_ward_20()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
